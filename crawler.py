# crawler.py
import requests
from bs4 import BeautifulSoup

BASE_URL = "https://lpjh.ylc.edu.tw"
HEADERS = {"User-Agent": "Mozilla/5.0"}

# å´™èƒŒåœ‹ä¸­åˆ†é¡å°æ‡‰
CATEGORY_URLS = {
    "æ ¡å‹™å¸ƒå‘Šæ¬„": f"{BASE_URL}/latest-news",
    "å…§éƒ¨å…¬å‘Š": f"{BASE_URL}/internal-news",
    "çå­¸é‡‘å…¬å‘Š": f"{BASE_URL}/scholarship",
    "å…¬æ–‡è½‰çŸ¥": f"{BASE_URL}/announcements",
    "æ‹›ç”Ÿå°ˆå€": f"{BASE_URL}/admissions",
    "æ•™å‹™è™•å…¬å‘Š": f"{BASE_URL}/academics",
    "å­¸å‹™è™•å…¬å‘Š": f"{BASE_URL}/students-affairs",
    "èª²å¾Œç¤¾åœ˜": f"{BASE_URL}/students-affairs",
}


def full_url(href):
    if not href:
        return None
    if href.startswith("http"):
        return href
    if href.startswith("/"):
        return BASE_URL + href
    return f"{BASE_URL}/{href}"


def fetch_page_items(url):
    """ æŠ“å–å…¬å‘Šï¼šæ—¥æœŸ + æ¨™é¡Œ + é€£çµ """
    try:
        r = requests.get(url, timeout=5, headers=HEADERS)
    except:
        return []

    soup = BeautifulSoup(r.text, "html.parser")

    items = []

    # ğŸ”¥ æ­£ç¢º selectorï¼šæŠ“æ‰€æœ‰å…¬å‘Š li
    li_list = soup.select("ul.list li")

    for li in li_list:
        # æ¨™é¡Œ
        a = li.find("a")
        if not a:
            continue

        title = a.get_text(strip=True)
        href = a.get("href")

        # æ—¥æœŸï¼ˆå¯æœ‰å¯ç„¡ï¼‰
        date_tag = li.find("span", class_="news-date")
        date = date_tag.get_text(strip=True) if date_tag else ""

        items.append({
            "title": f"{date} {title}",
            "url": full_url(href)
        })

    return items


def search_school(category: str, keyword: str = ""):

    url = CATEGORY_URLS.get(category)
    if not url:
        return []

    items = fetch_page_items(url)

    # é—œéµå­—éæ¿¾
    if keyword:
        items = [i for i in items if keyword in i["title"]]

    return items[:10]
