# crawler.py
import requests
from bs4 import BeautifulSoup

BASE_URL = "https://lpjh.ylc.edu.tw"

# ğŸ”¥ å¼·åŒ–ç‰ˆ headersï¼ˆå½è£æˆ Chromeï¼Œé¿å… Render è¢«æ“‹ï¼‰
HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Connection": "keep-alive",
}

# ğŸ”¥ å´™èƒŒåœ‹ä¸­åˆ†é¡å°æ‡‰
CATEGORY_URLS = {
    "æ ¡å‹™å¸ƒå‘Šæ¬„": f"{BASE_URL}/latest-news",
    "å…§éƒ¨å…¬å‘Š": f"{BASE_URL}/internal-news",
    "çå­¸é‡‘å…¬å‘Š": f"{BASE_URL}/scholarship",
    "å…¬æ–‡è½‰çŸ¥": f"{BASE_URL}/announcements",
    "æ‹›ç”Ÿå°ˆå€": f"{BASE_URL}/admissions",
    "æ•™å‹™è™•å…¬å‘Š": f"{BASE_URL}/academics",
    "å­¸å‹™è™•å…¬å‘Š": f"{BASE_URL}/students-affairs",
    "èª²å¾Œç¤¾åœ˜": f"{BASE_URL}/students-affairs",
}

# ğŸ”¥ å¿«é€Ÿé€£çµï¼ˆä½ å¯ä»¥è‡ªç”±æ–°å¢ï¼‰
QUICK_LINKS = {
    "å­¸å‹™ç³»çµ±": "https://www.ylc.edu.tw/staff-system",
    "å­¸æ ¡è¡Œäº‹æ›†": f"{BASE_URL}/calendar",
    "èª²è¡¨æŸ¥è©¢": f"{BASE_URL}/academics",
    "æ ¡åœ’é£Ÿæç™»å…¥": "https://fatrace.tw",
    "å…¨åœ‹åœ¨è·é€²ä¿®ç¶²": "https://www1.inservice.edu.tw",
    "å¸«ç”Ÿ e-mail": "https://mail.google.com",
}


# ----------------------------------------------------------
# URL è¼”åŠ©
# ----------------------------------------------------------
def full_url(href):
    if not href:
        return None
    if href.startswith("http"):
        return href
    if href.startswith("/"):
        return BASE_URL + href
    return f"{BASE_URL}/{href}"


# ----------------------------------------------------------
# æŠ“å–å…¬å‘Šï¼šæ—¥æœŸ + æ¨™é¡Œ + é€£çµ
# ----------------------------------------------------------
def fetch_page_items(url):
    try:
        r = requests.get(url, timeout=5, headers=HEADERS)
    except:
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    li_list = soup.select("ul.list li")   # ğŸ”¥ æ­£ç¢º selector

    items = []
    for li in li_list:
        a = li.find("a")
        if not a:
            continue

        title = a.get_text(strip=True)
        href = a.get("href")

        # æ—¥æœŸ
        date_tag = li.find("span", class_="news-date")
        date = date_tag.get_text(strip=True) if date_tag else ""

        items.append({
            "title": f"{date} {title}",
            "url": full_url(href)
        })

    return items


# ----------------------------------------------------------
# å–®åˆ†é¡æœå°‹
# ----------------------------------------------------------
def search_school(category: str, keyword: str = ""):
    url = CATEGORY_URLS.get(category)
    if not url:
        return []

    items = fetch_page_items(url)

    # é—œéµå­—éæ¿¾
    if keyword:
        items = [i for i in items if keyword in i["title"]]

    return items[:10]


# ----------------------------------------------------------
# å…¨åˆ†é¡æœå°‹ï¼ˆå…¨æ ¡å…¬å‘Šä¸€æ¬¡æœï¼‰
# ----------------------------------------------------------
def global_search(keyword):
    results = []

    for cat, url in CATEGORY_URLS.items():
        items = fetch_page_items(url)
        for i in items:
            if keyword in i["title"]:
                results.append(i)

    # å»é™¤é‡è¤‡ + é™åˆ¶å‰ 10 ç­†
    unique = []
    seen = set()

    for item in results:
        if item["title"] not in seen:
            seen.add(item["title"])
            unique.append(item)

    return unique[:10]
